version: "3"

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "5555:5555"
    environment:
      PERSIST_DIRECTORY: /data/db
      MODEL_TYPE: LlamaCpp
      MODEL_PATH: /data/models/ggml-vicuna-7b-1.1-q4_2.bin
      EMBEDDINGS_MODEL_NAME: all-MiniLM-L6-v2
      MODEL_N_CTX: 1000
      TARGET_SOURCE_CHUNKS: 4
    volumes:
      - ${PWD}/db:/data/db
      - ${PWD}/models:/data/models

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3333:3000"
    environment:
      NEXT_PUBLIC_BACKEND_URL: http://backend:5555/query
    depends_on:
      - backend

  ingest:
    build:
      context: ./ingest
      dockerfile: Dockerfile
    environment:
      PERSIST_DIRECTORY: /data/db
      EMBEDDINGS_MODEL_NAME: all-MiniLM-L6-v2
      MODEL_N_CTX: 1000
      TARGET_SOURCE_CHUNKS: 4
      SOURCE_DIRECTORY: /data/source_documents
    volumes:
      - ${PWD}/db:/data/db
      - ${PWD}/source_documents:/data/source_documents
